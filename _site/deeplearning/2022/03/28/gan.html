<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags-->
   <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Generative Adversarial Network with Bells and Whistle | Kiki’s Blog</title>
<meta name="generator" content="Jekyll v3.9.2" />
<meta property="og:title" content="Generative Adversarial Network with Bells and Whistle" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Dissecting GAN paper and it’s implementation" />
<meta property="og:description" content="Dissecting GAN paper and it’s implementation" />
<link rel="canonical" href="http://localhost:4000/deeplearning/2022/03/28/gan.html" />
<meta property="og:url" content="http://localhost:4000/deeplearning/2022/03/28/gan.html" />
<meta property="og:site_name" content="Kiki’s Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-03-28T00:00:00+07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Generative Adversarial Network with Bells and Whistle" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-03-28T00:00:00+07:00","datePublished":"2022-03-28T00:00:00+07:00","description":"Dissecting GAN paper and it’s implementation","headline":"Generative Adversarial Network with Bells and Whistle","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/deeplearning/2022/03/28/gan.html"},"url":"http://localhost:4000/deeplearning/2022/03/28/gan.html"}</script>
<!-- End Jekyll SEO tag -->
 


    <!-- Bootstrap-->
    <link href="/assets/css/bootstrap.min.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries-->
    <!-- WARNING: Respond.js doesn't work if you view the page via file://-->
    <!--if lt IE 9script(src='https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js')
script(src='https://oss.maxcdn.com/respond/1.4.2/respond.min.js')-->
    <!-- <link href="css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    <link rel="stylesheet" href="/assets/css/fontawesome.min.css">

    <link rel="stylesheet" href="/assets/css/style.css">

    <script>window.console = window.console || function (t) {
        };


    </script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css" />

    <!-- Katex -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body,{
    delimiters: [
      { left: '$$',  right: '$$',  display: true  },
      { left: '$',   right: '$',   display: false },
      { left: '\\[', right: '\\]', display: true  },
      { left: '\\(', right: '\\)', display: false }
  ]});"></script>
    
    <script src="/assets/js/template.v1.js"></script>

    <script src="/assets/js/d3.js"></script>
</head>
<link rel="stylesheet" href="/assets/css/diagram.css">
<link rel="stylesheet" href="/assets/css/diagram-g.css">
<body>
  <nav id="nav" class="navbar navbar-fixed-top">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header">
            <button type="button" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1"
                aria-expanded="false" class="navbar-toggle collapsed"><span class="sr-only">Toggle
                    navigation</span><span class="glyphicon glyphicon glyphicon-menu-hamburger"></span></button>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div id="bs-example-navbar-collapse-1" class="collapse navbar-collapse navbar-ex1-collapse">

            <ul class="nav navbar-nav">
                <li class="active"><a href="#ml" class="white-when-scroll">Deep Learning<span
                            class="sr-only">(current)</span></a>
                </li>
                <li><a href="https://kikirizki.github.io/index.html#crypt" class="white-when-scroll">Cryptography</a>
                </li>
                <li><a href="https://kikirizki.github.io/index.html#math" class="white-when-scroll">Probability
                        Theory</a></li>
                <li><a href="https://kikirizki.github.io/index.html#about" class="white-when-scroll">About
                        Me</a></li>
                <li><a href="https://github.com/kikirizki"><i class="fa-brands fa-github"></i></a></li>
                <li><a href="https://medium.com/@kkrzkrk"><i class="fa-brands fa-medium"></i></a></li>
            </ul>
        </div><!-- /.navbar-collapse -->
    </nav>

  <script type="text/front-matter">
    title: Generative Adversarial Network with Bells and Whistle
    description: Dissecting GAN paper and it's implementation
    authors:
    - Kiki Rizki Arpiandi: https://github.com/kikirizki
    
    affiliations:
    - My Medium articles: https://medium.com/@kkrzkrk
</script>

  <dt-article class="centered article-bg">
    <h1>Generative Adversarial Network with Bells and Whistle</h1>
    <h2>Dissecting GAN paper and it's implementation</h2>
    <dt-byline></dt-byline>
    <h2>Overview</h2>
    <p>This section will explain the general idea about GAN (Generative Adverserial Network)<dt-cite
        key="goodfellow2014generative"></dt-cite>. Broadly speaking from probabilistic perspective there are two kind of mathematical models;
      generative model and discriminative model, GAN as it's name suggest falls into the first cathegory, so here
      we will focus on generative model. To make things easier to understand let's make an analogy,
      let say I have data in the form of images for specific the data is a collection of some of Pablo Picasso
      paintings, the generative model is a model that approximate the probability distribution that generate the
      paintings, such that
      the model able generate a new paintings that look like Pablo Picasso painting, but have not seen
      before. Each pixel data (normalized so that the values are between 0 and 1) in the paintings can be
      represented as vector of probabilities that is generated by $\mathbb{P}_{data}$ which is an unknown
      probability distribution
      and we wish to approximate it.
    </p>


    <p>
      We will approximate $\mathbb{P}_{data}$ with a parametric function $G(z,\theta_g)$
      which is a function of random variable $Z \sim \mathbb{P}_z$ and has distribution $\mathbb{P}_{g}$. The authors of GAN call $G(z,\theta_g)$ as generator and the
      function is in the form of artificial neural network.
      There are many ways to make generator generates sample as close as possible to samples that are generated by
      $\mathbb{P}_{data}$ for instance we can set a loss function between our generator's samples and
      real
      data then minimize the loss function with somekind of optimization algortihm, this is kind of method is a
      direct
      approach to the problem, instead of using direct approach GAN framework is inspired by min-max game from
      game
      theory,
      by introducing a new function called discriminator $D(x)$ with distribution $\mathbb{P}_{\text{is real}}$. Discriminator inputs are samples
      from $\mathbb{P}_{data}$ and $\mathbb{P}_{g}$ and return the probability of input
      was generated by $\mathbb{P}_{data}$, so the perfect discrimiator will return 1.0 if the input
      comes from real data and return 0.0 if input comes from generator, we can also consider discriminator as an
      "ordinary binary classifier" which classify wheter a data is real or fake. On the other hand generator tries
      to generate
      samples to look as close as possible to real data in order to outsmart the discriminator or in other word it
      try to make
      discriminator to classify an input as real whenever the input is fake (generated by generator). More
      formally
      discriminator and generator aims are as follow:
    <p>
      <b>Discriminator Task</b>:
    <ul>
      <li> Maximize the value of $\mathbb{P}_\text{is real}(x_1,x_2,\dots,x_n)$ where $x_1,x_2,\dots,x_n$ are samples
        from real dataset </li>
      <li> Minimize the value of $\mathbb{P}_\text{is real}(G(z_1),G(z_2),\dots,G(z_n))$ where $G(z_1),G(z_2),\dots,G(z_n)$ are
        samples generated by generator</li>
    </ul>
    </p>
    <p>
      <b>Generator Task</b>:
    <ul>
      <li> Maximize the value of $\mathbb{P}_\text{is real}(G(z_1),G(z_2),\dots,G(z_n))$ </li>
    </ul>
    </p>
    <p>
      From the list above we can see that generator and discriminator competing to each other, this raise some
      important questions, for instance how come this form
      of competing model achieve our main goal a.k.a make a model that can generate samples that look like real
      data. When we suppose to stop the training ?. To answer those questions
      we will start by defining how our criterion should look like.
    </p>
    <h2>Setting up the Criterion</h2>
    <p>
      Let's begin with the first task of discriminator, Suppose that $x_1,x_2,\dots,x_n$ is our real
      samples we wish to maximize $\mathbb{P}_\text{is real}(x_1,x_2,\dots,x_n)=\prod_{1}^{n}\mathbb{P}_\text{is real}(x_i)=\prod_{1}^{n} D(x_i)$, but from computation point of view choosing this
      value as criterion is a bad
      decision since it saturate small probability value to zero, here is a code snippet that illustrate the
      phenomena
    </p>
    <dt-code block language="python">
      import numpy as np

      r1 = np.random.random_sample((100,)) * 0.00001
      r2 = r1*0.1
      r3 = r2*0.1
      l1 = [np.log(x) for x in r1]
      l2 = [np.log(x) for x in r2]
      l3 = [np.log(x) for x in r3]
      print(f"Product of probability : {np.product(r1)}")
      print(f"Product of probability : {np.product(r2)}")
      print(f"Product of probability : {np.product(r3)}")
      print(f"Sum of probability : {np.sum(l1)}")
      print(f"Sum of probability : {np.sum(l2)}")
      print(f"Sum of probability : {np.sum(l3)}")

      # Output :
      # Product of probability : 0.0
      # Product of probability : 0.0
      # Product of probability : 0.0
      # Sum of probability : -1248.714287675051
      # Sum of probability : -1478.9727969744558
      # Sum of probability : -1709.2313062738604

    </dt-code>
    <p>
      From the output of the code above, we can see that small values equal to zero after multipication, which is
      obviously not the exact value, this is caused by the decimal rounding computer does,
      and since we multiply many smalls values that are rounded down the result become extremely small as the
      result it close to zero then computer round it to zero, as an alternative
      since logarithm function is strictly increasing function hence $\log\left[\prod_{1}^{n} D(x_i)\right]=\sum_{1}^{n} \log D(x_i)$ will serve
      the same purpose, since it is only the scaled version of $\prod_{1}^{n} D(x_i)$. In statistics it is
      usually called log-likelihood, and since the dataset can be considered as generated by empirical
      distribution, we can express the log-likelihood
      in term of expectation,
    </p>

    $$\begin{aligned}\sum_{1}^{n} \log D(x_i)&=n\sum_{1}^{n}\frac{1}{n} \log D(x_i) \\ &= n\sum_{1}^{n} \mathbb{P}_{data}(x_i) \log D(x_i) \\ &=n\mathbb{E}_{x\sim \mathbb{P}_{data}}\left[log(D(x))\right] \end{aligned}$$
    <p>
      With the same line of reasoning for the second task of discriminator we want to maximize the value of $n\mathbb{E}_{z\sim \mathbb{P}_g}\left[1-log(D(G(z)))\right]$, note here we turn minimizing task into maximizing task, so to do
      both task we with to maximize
    </p>
    $$\begin{aligned}V^{\prime}(D,G)=n\mathbb{E}_{x\sim \mathbb{P}_{data}}\left[log(D(x))\right]+n\mathbb{E}_{z\sim \mathbb{P}_z}\left[\log(1-D(G(z)))\right]\end{aligned}$$
    <p>Since $n$ is only a scaler we can drop $n$ such that we want to maximize
    </p>
    $$\begin{aligned}V(D,G)=\mathbb{E}_{x\sim \mathbb{P}_{data}}\left[log(D(x))\right]+\mathbb{E}_{z\sim \mathbb{P}_z}\left[\log(1-D(G(z)))\right]\end{aligned}$$

    <p>Now move to the goal of generator, if we denote optimal discriminator as $D^* = \text{argmax}_D V(G,D)$ then
      the goal of generator is to minimize</p>
      $$\begin{aligned}V(D^{*},G)=\mathbb{E}_{x\sim \mathbb{P}_{data}}\left[\text{log}(D^{*}(x))\right]+\mathbb{E}_{z\sim \mathbb{P}_z}\left[\log(1-D^{*}(G(z)))\right]\end{aligned}$$

    <h2>Optimal Discriminator</h2>
    <p>In this section we aim to find $D^{*}(x)= \text{argmax}_{D}V(D,G)$. By the Law of Unconcious Statistician<dt-cite
        key="lotus-proof"></dt-cite> we get this equation </p>
    $$\mathbb{E}_{z\sim \mathbb{P}_z}\left[\log (1-D(G(z)))\right]=\mathbb{E}_{x\sim \mathbb{P}_g}\left[\log (1-D(x))\right]$$
    <p>hence</p>
    $$\begin{aligned} V(D,G)&=\mathbb{E}_{x\sim \mathbb{P}_{data}}\left[\log D(x)\right]+\mathbb{E}_{z\sim \mathbb{P}_z}\left[\log  (1-D(G(z)))\right] \\ &=\mathbb{E}_{x\sim \mathbb{P}_{data}}\left[\log D(x)\right]+\mathbb{E}_{x\sim \mathbb{P}_g}\left[\log (1- D(x))\right] \\ &= \int_{x}\log D(x)\mathbb{P}_{data}(x)dx+\int_{x}\log (1-D(x))\mathbb{P}_g(x)dx \\ &=\int_{x}\log D(x)\mathbb{P}_{data}(x) + \log (1-D(x))\mathbb{P}_g(x)dx \end{aligned}$$
    <p>Now we want to find the value of that maximize the integran by using elementary calculus,i.e setup the
      derivation to zero, as follow:</p>
    $$\Longleftrightarrow \frac{d\log D(x)\mathbb{P}_{data}(x) + \log (1-D(x))\mathbb{P}_g(x)}{d D(x)}=\frac{\mathbb{P}_{data}(x)}{D(x)}-\frac{\mathbb{P}_g(x)}{1-D(x)}=0 \\ \Longleftrightarrow \mathbb{P}_{data}(x)=D(x)\mathbb{P}_g(x)+D(x)\mathbb{P}_{data}(x) \\ \Longleftrightarrow D(x)=\frac{\mathbb{P}_{data}(x)}{\mathbb{P}_g(x)+\mathbb{P}_{data}(x)}$$
    <p>So we get the optimal value for discriminator i.e $D^{*}(x)=\frac{\mathbb{P}_{data}(x)}{\mathbb{P}_g(x)+\mathbb{P}_{data}(x)}$</p>
    <h2>Optimal Generator</h2>
    <p>Now we have an optimal discriminator, next we want to find $G^* = \text{argmax}_G V(G,D^{*})$ but let's first expand
      $V(G,D^{*})$ :
    </p>
    $$\begin{aligned}V(G,D^{*})&=\int_{x}\log\left[\frac{\mathbb{P}_{data}(x)}{\mathbb{P}_g(x)+\mathbb{P}_{data}(x)}\right]\mathbb{P}_{data}(x)+\log\left[1-\frac{\mathbb{P}_{data}(x)}{\mathbb{P}_g(x)+\mathbb{P}_{data}(x)}\right]\mathbb{P}_g(x) dx \\ &=\int_{x}\log\left[\frac{2\mathbb{P}_{data}(x)}{2(\mathbb{P}_g(x)+\mathbb{P}_{data}(x))}\right]\mathbb{P}_{data}(x)dx+\int_{x}\log\left[\frac{2\mathbb{P}_g(x)}{2(\mathbb{P}_g(x)+\mathbb{P}_{data}(x))}\right]\mathbb{P}_g(x)dx \\ &= -\log 2 + \int_{x}\log\left[\frac{2\mathbb{P}_{data}(x)}{(\mathbb{P}_g(x)+\mathbb{P}_{data}(x))}\right]\mathbb{P}_{data}(x)dx-\log 2 + \int_{x}\log\left[\frac{2\mathbb{P}_g(x)}{(\mathbb{P}_g(x)+\mathbb{P}_{data}(x))}\right]\mathbb{P}_g(x)dx \\ &= -\log 4+2\mathbf{D}_{\text{Jensen-Shanon}}(P_g||P_{data})\end{aligned}$$
    <p> So here we have nice interpretation because it is easy to find the minimium now, the minimum value of
      Jensen-Shanon divergence is atained when $\mathbb{P}_{data}(x)=\mathbb{P}_g(x)$ which cause $D^{*}(x)=\frac{\mathbb{P}_{data}(x)}{\mathbb{P}_g(x)+\mathbb{P}_{data}(x)}=\frac{1}{2}$ in other words when the min-max game equilibrium between generator and
      discriminator achieved, discriminator always return 0.5 for both fake and real data, which mean the
      discriminator is confused and cannot tell the difference between real and fake data. So theoritically
      the best time to stop the training is when discriminator return 50% confidence for both samples that
      generated by generator or real data, but this is only half of the story, we cannot guarantee that our neural
      network model or our optimization algortihm can achieve that condition.</p>
    <h2>Training GAN</h2>
    <p>Training GAN is somekind of art as it is not as straight forward as training "conventional" neural network
      architecture. The original paper of GAN itself does not provide detail about training procedure but more
      like general framework and the detail is left to us. Since
      GAN is very popular many researchers and engineers working on it, there is some nice article out there the
      provide technique to train GAN effectively
      such as ganhack ganhack<dt-cite key="ganhack"></dt-cite>, although ganhack is quite outdated but we will use
      some tips from them
      to train our GAN implementation here.</p>
    <h3>Training Discriminator</h3>
    <p> Remember that our discriminator goal is to maximize $\mathbb{E}_{x\sim \mathbb{P}_{data}}\left[\log(D(x))\right]+\mathbb{E}_{z\sim \mathbb{P}_z}\left[\log(1-D(G(z)))\right]$ notice that we can turn
      is maximizing problem into minimizing binary cross entropy loss and the
      sthocastic version (sum over batch) will look like this $L_{\text{BCE}}=-\sum^{n}_{i=1}y_i\log{\hat{y}_i}+(1-y_i)\log(1-\hat{y}_i)$ 
      according
      to ganhack tips it is better to split the batch between real batch and fake batch. Feed forward
      the fake-only batch compute the loss, feed forward the real-only batch compute the loss, sum up both loss
      from real-only and fake-only batch
      then compute the gradient, finally update weight. As for illustration below is diagram of discrimiator
      forward and backward propagation. The <b><span style="color:#00bcd4;">blue</span></b> line is the flow of
      forward propagation from real data to summing operator, and the <b><span style="color:#ffc107">yellow</span></b>
      is from fake data,
      the <b><span style="color:#f44336">red</span></b> line is the flow of gradients from summed loss to
      discriminator.
    </p>
    <div id="discriminator"> </div>
    <h3>Training Generator</h3>
    <p>Training generator is straight forward since we will only feed noise. The quirk is only we will not minimize
      $\mathbb{E}_{z\sim \mathbb{P}_z}\left[\log(1-D(G(z)))\right]$ rather than maximize $\mathbb{E}_{x\sim \mathbb{P}_{data}}\left[\log(D(G(z)))\right]$ hence we should assign
      1.0 as label (true label) for generator output <b>during training the generator</b>
    </p>
    <div id="generator"> </div>
    <h2>GAN inference</h2>
    <p>The inference is very straigh forward, just ignore the discrimiator part, generate noise and feed it to
      generator</p>
    <h2>Implementation</h2>
    <p>The code implementation is in python <i class="fa-brands fa-python"></i> I have tried to make the code as
      readable as possible by minimizing the boilerplates,
      we will use pytorch framework to help with the automatic differentiation part for backpropagation which is
      not essential in this article, also the neural network model for
      discrimiator and generator are mlp with super simple architecture, and the BCE loss is written explicitly to
      enhance understanding, but
      in practice I really recommend to use <dt-code>nn.BCELoss()</dt-code> from PyTorch since it is well optimized and
      battle
      tested.

    </p>
    <p>Import needed library, and use manual seed so we will get the same result if we run it several times
      <dt-code block language="python">
        from __future__ import print_function
        #%matplotlib inline
        import torch
        import torch.nn as nn
        import torch.optim as optim
        from torchvision import transforms
        from torchvision.datasets import MNIST
        from torch.utils.data import DataLoader
        import imageio
        import random
        import matplotlib.pyplot as plt
        import numpy as np
        # Set random seed for reproducibility
        manualSeed = 999
        random.seed(manualSeed)
        torch.manual_seed(manualSeed)
      </dt-code>

    </p>
    <p>
      We will use MNIST dataset, for ease of use let use the one provided by pytorch
      <dt-code block language="python">
        transform = transforms.Compose([
                        transforms.ToTensor(),
                        transforms.Normalize((0.5,),(0.5,))
                        ])
        to_image = transforms.ToPILImage()
        trainset = MNIST(root='./data/', train=True, download=True, transform=transform)
        trainloader = DataLoader(trainset, batch_size=100, shuffle=True)
        
        device = 'cuda'
                    </dt-code>
        

    </p>
    <p>
      Now define our simple generator and discrimiator architecture
      <dt-code block language="python">
class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        self.n_features = 128
        self.n_out = 784
        self.mlp = nn.Sequential(
                   nn.Linear(self.n_features, 256),
                   nn.LeakyReLU(0.2),
                   nn.Linear(256, 512),
                   nn.LeakyReLU(0.2),
                   nn.Linear(512, 1024),
                   nn.LeakyReLU(0.2),
                   nn.Linear(1024, self.n_out),
                   nn.Tanh())
    def forward(self, x):
        x = self.mlp(x)
        x = x.view(-1, 1, 28, 28)
        return x
    
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.n_in = 784
        self.n_out = 1
        self.mlp = nn.Sequential(
                     nn.Linear(self.n_in, 1024),
                     nn.LeakyReLU(0.2),
                     nn.Dropout(0.3),
                     nn.Linear(1024, 512),
                     nn.LeakyReLU(0.2),
                     nn.Dropout(0.3),
                     nn.Linear(512, 256),
                     nn.LeakyReLU(0.2),
                     nn.Dropout(0.3),
                     nn.Linear(256, self.n_out),
                     nn.Sigmoid())
    
    
    def forward(self, x):
        x = x.view(-1, 784)
        x = self.mlp(x)
        return x
      </dt-code>
    </p>
    <p>Define our hyperparameter
      <dt-code block language="python">
        num_epochs = 50
        lr = 0.0002
        beta1 = 0.5
        ngpu = 1
        latent_variable_dim =128
      </dt-code>
    </p>
    <p>Define loss function and optimizer, here in practice it is possible that discrimiator return zero and log at zero
      is undefined, to fix this we will add very small number <dt-code>epsilon = 1e-10 </dt-code>, to the output of
      discriminator
      <dt-code block language="python">
        def binary_cross_entropy(pred, y):
            epsilon =  1e-10 
            return -((pred+epsilon).log()*y + (1-y)*(1-pred+epsilon).log()).mean()

        generator_model = Generator().to(device)
        discriminator_model = Discriminator().to(device)

        # Establish convention for real and fake labels during training
        real_label = 1.
        fake_label = 0.

        discriminator_optimizer = optim.Adam(discriminator_model.parameters(), lr=lr, betas=(beta1, 0.999))
        generator_optimizer = optim.Adam(generator_model.parameters(), lr=lr, betas=(beta1, 0.999))

      </dt-code>

    </p>
    <p>
      Write function for viewing the generated results
      <dt-code block language="python">
        def view_result(generator_net, batch = 8):
            # Create batch of latent vectors that we will use to visualize
            noise = torch.randn((batch,128), device=device)
            with torch.no_grad():
                output = generator_net(noise)
            #re-arange [NxCxHxW] to [NxHxW] since channel is 1 we will squeeze it
            output = output.permute(1,0,2,3).squeeze()
            output_np = output.cpu().detach().numpy()
            output_np = np.hstack(output_np)
            print(output_np.shape)
            plt.imshow(output_np)

      </dt-code>
    </p>
    <p>
      Train our GAN and wait for some minutes
      <dt-code block language="python">
          img_list = []
          G_losses = []
          D_losses = []
          iters = 0
          
          print("Starting Training Loop...")
          # For each epoch
          for epoch in range(num_epochs):
              # For each batch in the dataloader
              for i, data in enumerate(trainloader, 0):
          
                  # ------------------------------------------------------------------#
                  # Update Discriminator model: maximize log(D(x)) + log(1 - D(G(z))) #
                  # ------------------------------------------------------------------#
                 
                  # Train with batch from real data
                  discriminator_model.zero_grad()
                  real_cpu = data[0].to(device)
                  b_size = real_cpu.size(0)
                  label = torch.full((b_size,), real_label, dtype=torch.float, device=device)
                  # Forward real data batch through D
                  output = discriminator_model(real_cpu).view(-1)
                  # Calculate loss binary cross entropy for real batch
                  errD_real = binary_cross_entropy(output, label)
                  # Calculate discriminator gradient
                  errD_real.backward()
                  D_x = output.mean().item()
          
                  ## Train with samples generated by generator
                  # Generate batch of latent vectors
                  noise = torch.randn((b_size,128), device=device)
                  # Generate batch of fake images using generator 
                  fake = generator_model(noise)
               
                  label.fill_(fake_label)
                  # Classify all fake images batch with discriminator
                  output = discriminator_model(fake.detach()).view(-1)
                  # Calculate D's loss on the all fake images batch
                 
                  errD_fake = binary_cross_entropy(output, label)
                  # Calculate the gradients for this batch, accumulated (summed) with previous gradients
                  errD_fake.backward()
                  D_G_z1 = output.mean().item()
                  # Compute discriminator error (loss) as sum over the fake and the real batches
                  errD = errD_real + errD_fake
                  # Update discriminator
                  discriminator_optimizer.step()
          
                  #------------------------------------------------#
                  # Update Generator model : maximize log(D(G(z))) #
                  #------------------------------------------------#
                  generator_model.zero_grad()
                  label.fill_(real_label)  # fake labels are real for generator cost
                  # Since we just updated discriminator, perform another forward pass of all-fake batch through D
                  output = discriminator_model(fake).view(-1)
                  # Calculate Generator's loss based on this output
                  errG = binary_cross_entropy(output, label)
                  # Calculate generator gradients 
                  errG.backward()
                  D_G_z2 = output.mean().item()
                  # Update generator
                  generator_optimizer.step()
          
                  # Print training loss, D(x) and D(G(z))
                  if i % 50 == 0:
                      print(f"Epoch :{epoch+1}/{num_epochs} Loss_D {errD.item():.2f} Loss_G {errG.item():.2f} D(x): {D_x:.2f} D(G(z)): {D_G_z1:.2f}/{D_G_z2:.2f}")
                    
                # Save Losses for plotting later
              G_losses.append(errG.item())
              D_losses.append(errD.item())</dt-code>
  </p>
    <p>
      View the result, if you run the code correctly after the training is finished then we can see the loss plot and
      some generated result by running the code snippet bellow
      <dt-code block language="python">
        plt.figure(figsize=(10,5))
        plt.title("Generator and Discriminator Loss During Training")
        plt.plot(G_losses,label="G")
        plt.plot(D_losses,label="D")
        plt.xlabel("epochs")
        plt.ylabel("Loss")
        plt.legend()
        plt.show()
        view_result(generator_model,batch=8)
      </dt-code>
      the plot and generated image should look like This


    </p>
    <p style="align-content: center">
    <figure class="grid" style="grid-template-columns: 3fr 1fr;">
      <img src="/assets/images/gan_training_graph.png" style="max-width: 400px">
      <figcaption>Plot of discrimiator and generator loss function, the loss/error of discriminator increased due to
        generator is getting better
      </figcaption>
    </figure>
    <figure class="grid" style="grid-template-columns: 3fr 1fr;">
      <img src="/assets/images/gan_result.png" style="max-width: 400px">
      <figcaption>Generated samples from generator
      </figcaption>
    </figure>
    </p>




  </dt-article>

  <!-- Appendix -->
  
    <dt-appendix class="centered">
<h3 id="citation">Errors and Correction</h3>
<p>Please email me at kkrzkrk@gmail.com</p>
<h3 id="citation">Citations and Reuse</h3>
<p>Diagrams and text are licensed under Creative Commons Attribution <a
        href="https://creativecommons.org/licenses/by/2.0/">CC-BY 2.0</a>. The figures that have been reused
    from other sources don't fall under this license and can be recognized by a note in their caption: “Figure
    from …”.</p>
<p>For attribution in academic contexts, please cite this work as</p>
<pre class="citation short">Arpiandi, Kiki Rizki, "Generative Adversarial Network with Bells and Whistle", 2022.</pre>

<p>BibTeX citation</p>

<pre class="citation long">@article
{ 
  kiki2022gan,
  author = {Arpiandi, Kiki Rizki},
  title = { Generative Adversarial Network with Bells and Whistle },
  year = {2022},
  url = {https://kikirizki.github.io/gan.html}
}</pre>
</dt-appendix>

  <script type="text/bibliography">
  @article{DBLP:journals/corr/abs-1912-05911,
  author    = {Robin M. Schmidt},
  title     = {Recurrent Neural Networks (RNNs): {A} gentle Introduction and Overview},
  journal   = {CoRR},
  volume    = {abs/1912.05911},
  year      = {2019},
  url       = {http://arxiv.org/abs/1912.05911},
  eprinttype = {arXiv},
  eprint    = {1912.05911},
  timestamp = {Thu, 02 Jan 2020 18:08:18 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1912-05911.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{kiki-inverse-transform,
    author    = {Kiki Rizki Arpiandi},
    title     = {Inverse Transform Method in Python},
    year      = {2022},
    url       = {https://kikirizki.github.io/inverse-transform.html}
  }
  @article{lotus-proof,
    author = {Joram Soch}, 
    title = {The Book of Statistical Proofs}, 
    author = {Thomas J. Faulkenberry and Kenneth Petrykowski and Carsten Allefeld},
    year = {2020},
    eprint= {StatProofBook/StatProofBook.github.io: StatProofBook 2020 (Version 2020)},
    doi =  {10.5281/zenodo.4305950},
    url = {https://statproofbook.github.io/P/mean-lotus.html}

}
@misc{goodfellow2014generative,
    title={Generative Adversarial Networks}, 
    author={Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
    year={2014},
    eprint={1406.2661},
    archivePrefix={arXiv},
    primaryClass={stat.ML},
    url={https://arxiv.org/abs/1406.2661}
}
@misc{ganhack,
    title={How to Train a GAN? Tips and tricks to make GANs work},
    url={https://github.com/soumith/ganhacks},
    author={Soumith Chintala and Emily Denton and Martin Arjovsky and Michael Mathieu}
}
</script>
<script src="/assets/js/diagram.js"></script>
<script src="/assets/js/diagram-g.js"></script>
     <!-- jQuery (necessary for Bootstrap's JavaScript plugins)-->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed-->
    <script src="/assets/js/bootstrap.min.js"></script>
    <script src="/assets/js/scroll.js"></script>

</body>

</html>