<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags-->
   <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Transforming Simple Distribution with Normalizing Flow | Kiki’s Blog</title>
<meta name="generator" content="Jekyll v3.9.2" />
<meta property="og:title" content="Transforming Simple Distribution with Normalizing Flow" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="build up theoritical understanding of normalizing flow model" />
<meta property="og:description" content="build up theoritical understanding of normalizing flow model" />
<link rel="canonical" href="http://localhost:4000/probability%20theory/2022/05/26/normalizingflow.html" />
<meta property="og:url" content="http://localhost:4000/probability%20theory/2022/05/26/normalizingflow.html" />
<meta property="og:site_name" content="Kiki’s Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-05-26T00:00:00+07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Transforming Simple Distribution with Normalizing Flow" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-05-26T00:00:00+07:00","datePublished":"2022-05-26T00:00:00+07:00","description":"build up theoritical understanding of normalizing flow model","headline":"Transforming Simple Distribution with Normalizing Flow","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/probability%20theory/2022/05/26/normalizingflow.html"},"url":"http://localhost:4000/probability%20theory/2022/05/26/normalizingflow.html"}</script>
<!-- End Jekyll SEO tag -->
 


    <!-- Bootstrap-->
    <link href="/assets/css/bootstrap.min.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries-->
    <!-- WARNING: Respond.js doesn't work if you view the page via file://-->
    <!--if lt IE 9script(src='https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js')
script(src='https://oss.maxcdn.com/respond/1.4.2/respond.min.js')-->
    <!-- <link href="css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    <link rel="stylesheet" href="/assets/css/fontawesome.min.css">

    <link rel="stylesheet" href="/assets/css/style.css">

    <script>window.console = window.console || function (t) {
        };


    </script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css" />

    <!-- Katex -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body,{
    delimiters: [
      { left: '$$',  right: '$$',  display: true  },
      { left: '$',   right: '$',   display: false },
      { left: '\\[', right: '\\]', display: true  },
      { left: '\\(', right: '\\)', display: false }
  ]});"></script>
    
    <script src="/assets/js/template.v1.js"></script>

    <script src="/assets/js/d3.js"></script>
</head>

<body>
  <nav id="nav" class="navbar navbar-fixed-top">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header">
            <button type="button" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1"
                aria-expanded="false" class="navbar-toggle collapsed"><span class="sr-only">Toggle
                    navigation</span><span class="glyphicon glyphicon glyphicon-menu-hamburger"></span></button>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div id="bs-example-navbar-collapse-1" class="collapse navbar-collapse navbar-ex1-collapse">

            <ul class="nav navbar-nav">
                <li class="active"><a href="#ml" class="white-when-scroll">Deep Learning<span
                            class="sr-only">(current)</span></a>
                </li>
                <li><a href="https://kikirizki.github.io/index.html#crypt" class="white-when-scroll">Cryptography</a>
                </li>
                <li><a href="https://kikirizki.github.io/index.html#math" class="white-when-scroll">Probability
                        Theory</a></li>
                <li><a href="https://kikirizki.github.io/index.html#about" class="white-when-scroll">About
                        Me</a></li>
                <li><a href="https://github.com/kikirizki"><i class="fa-brands fa-github"></i></a></li>
                <li><a href="https://medium.com/@kkrzkrk"><i class="fa-brands fa-medium"></i></a></li>
            </ul>
        </div><!-- /.navbar-collapse -->
    </nav>

  <script type="text/front-matter">
    title: Transforming Simple Distribution with Normalizing Flow
    description: build up theoritical understanding of normalizing flow model
    authors:
    - Kiki Rizki Arpiandi: https://github.com/kikirizki
    
    affiliations:
    - My Medium articles: https://medium.com/@kkrzkrk
</script>


  <dt-article class="centered article-bg">

    <h1>Transforming Simple Distribution with Normalizing Flow</h1>
    <h2>build up theoritical understanding of normalizing flow model</h2>
    <dt-byline></dt-byline>
    <h2>Overview</h2>
    <p>Modeling a probability distribution has broad application in image generation, video interpolation, file compression an so on. In this article we will talk about one of interesting method to model a probability distribution called normalizing flow. In a nutshell, normalizing flow is a method to model a probability distribution by deforming a random variable to a simple predefined independent prior distribution, such as Gaussian to a more complex one. Given a random variable $\boldsymbol{X}\in \mathbb{R}^D$  from unknown complex distribution, normalizing flow model the distribution by train an  invertible parametric transformation  $f$ that map $\boldsymbol{X}$to $\boldsymbol{Z}$. During inference to get sample $\boldymbol{x}$, simply sample $\boldsymbol{z}$ then transform it by  $f^{-1}$. To more precise, let $\boldsymbol{x}, \boldsymbol{z}$ are samples from $\boldsymbol{X}$ and $\boldsymbol{Y}$  by change variable theorem : </p>
    <p>
      $p_X(\boldsymbol{x}) = p_Z(f(\boldsymbol{x}))|\text{det}\frac{\partial f(\boldsymbol{x})}{\partial \boldsymbol{x}}|
$
    </p>
   
    <p>
      This way it is easy to sample $\boldsymbol{x}$, by use this following step:
      
       $\begin{aligned}\boldsymbol{z}&\sim  p_Z(\boldsymbol{z})\\ \boldsymbol{x}&=f^{-1}(\boldsymbol{z})\end{aligned}$
      
      To add expressiveness of the model,  $f$ is designed to be a composition of invertible functions
      
      $f=f_S\circ f_n\circ f_{n-1}\circ f_{n-2},\dots, f_1$ and let $\boldsymbol{y}_0=\boldsymbol{x},  \boldsymbol{y}_i=f_i(\boldsymbol{y}_{i-1})$. $f_S$ will be special layer that give relative importance the the output, we will talk more about this in next section. By the change variable rule we have
    </p>
    <p>
      $$p_X(\boldsymbol{x}) = p_{Y_n}(\boldsymbol{y}_n)\left|\text{det}\frac{\partial \boldsymbol{y}_n}{\partial \boldsymbol{x}}\right|=p_{Y_n}(\boldsymbol{y}_n)\Pi_{i=1}^{n}\left|\text{det}\frac{\partial \boldsymbol{y}_i}{\partial\boldsymbol{y_{i-1}}}\right|$$
    </p>
    <h2>General Coupling Layer</h2>
    <p>The function $f_i$ map  $\boldsymbol{y}_{i-1}$ to $\boldsymbol{y}_i$. Coupling layer is one choice of $f_i$ that was proposed in paper NICE<dt-cite
      key="https://doi.org/10.48550/arxiv.1410.8516"></dt-cite> the idea is to design $f_i$ such that Jacobians are easy to compute. Before going further for simplicity let just think about the first layer and drop the subscript from $\boldsymbol{y}_i$ to $\boldsymbol{y}$. The idea begin by splitting $\boldsymbol{x}$ into two partitions $\boldsymbol{x}_{A}\in \mathbb{R}^{d} $ and $\boldsymbol{x}_{B}\in \mathbb{R}^{D-d}$. then  apply the following mapping</p>
    <p>$$\begin{aligned}\boldsymbol{y}_{A}&= \boldsymbol{x}_{A}\\
      \boldsymbol{y}_{B}&= g(\boldsymbol{x}_{B};m(\boldsymbol{x}_{A}))\end{aligned}$$</p>
    <p>Above mapping is called coupling layer with arbitrary function $m$. The choice of $m$ can be anything such as artificial neural network. </p>
    
    <p>It is easy to show that</p>
    <p>$$\frac{\partial \boldsymbol{y}}{\partial \boldsymbol{x}} = \begin{bmatrix}\boldsymbol{I}_d & \boldsymbol{0}_{d\times D-d} \\ \frac{\partial{\boldsymbol{y}_{B}}}{\partial \boldsymbol{x}_{A}} & \frac{\partial \boldsymbol{y}_{B}}{\partial \boldsymbol{x}_{B}}\end{bmatrix}$$</p>
    <h2>Additive Coupling Layer</h2>
    <p>Additive coupling layer is the coupling layer with</p>
    <p>$$g(\boldsymbol{x}_{B};m(\boldsymbol{x}_{A})) = \boldsymbol{x}_{B}+ m(\boldsymbol{x}_{A})$$</p>
    <p>since $\frac{\partial \boldsymbol{y}_{B}}{\partial \boldsymbol{x}_{B}}=\frac{\partial g(\boldsymbol{x}_{B};m(\boldsymbol{x}_{A}))}{\partial \boldsymbol{x}_B}$ hence the jacobian become</p>
    <p>$$\frac{\partial \boldsymbol{y}}{\partial \boldsymbol{x}}= \begin{bmatrix}I_d & \boldsymbol{0}_{d\times{D-d}} \\ \frac{\partial{\boldsymbol{y}_{B}}}{\partial \boldsymbol{x}_{A}} & I_{D-d} \end{bmatrix}$$</p>
    <p>Which is a lower triangular matrix, now we can easily compute jacobian $J=\left| \text{det}\frac{\partial \boldsymbol{y}}{\partial \boldsymbol{x}} \right|$ . Remember from linear algebra determinant of lower triangular matrix are the product of it’s diagonal entries (if you forget it please read your linear algebra book) and since our all diagonal entries equals to $1$ so $J=1$. It is easy to see recursively that $p_X(\boldsymbol{x}) = p_{Y_{n}}(\boldsymbol{y}_n)$.  </p>
    <p>Since one part of coupling layer input partition is left unchanged (mapped to identity function), when composing coupling layers we should change the role  between those two input partitions. To add more expressiveness to the model on the top of layer, multiply  $\boldsymbol{y}_n$ with learnable diagonal matrix $\boldsymbol{S}\in \mathbb{R}^{DxD}$  , this way it allow to control relative importance for each elements of $\boldsymbol{y}_n$ so the final layer become $\begin{aligned}\boldsymbol{z}&=\boldsymbol{S}\boldsymbol{y}_n \end{aligned}$ . From vector calculus it is easy to show that $\frac{\partial \boldsymbol{z}}{\partial \boldsymbol{y}_n}=\boldsymbol{S}$. This also cause $\frac{\partial \boldsymbol{z}}{\partial \boldsymbol{x}}=\frac{\partial \boldsymbol{z}}{\partial \boldsymbol{y}_n}\frac{\partial \boldsymbol{y}_n}{\partial \boldsymbol{x}}=\boldsymbol{S}\boldsymbol{I}_D=\boldsymbol{S}$ so we get final equation as follow

      $$p_X(\boldsymbol{x}) = p_{Z}(\boldsymbol{z})\left|\text{det}\frac{\partial  \boldsymbol{z}}{\partial \boldsymbol{x}}\right|=p_{Z}(\boldsymbol{z})\left|\text{det}\boldsymbol{S}\right|$$</p>
    <h2>Loss Function</h2>
    <p>The loss function is simply a log likelihood of our unknown distribution that generate our dataset$\text{log} (p_X(\boldsymbol{x}))$ and since prior distribution $p_Z(\boldsymbol{z})$ is independent then the loss function can be factorized into:</p>
    <p>$$\begin{aligned}\text{log} (p_X(\boldsymbol{x}))&= \text{log}(p_Z(\boldsymbol{z}))+\text{log}\left|\text{det}\boldsymbol{S}\right|\\&=\sum_{i=1}^{D}\text{log}(p_{Z_i}(\boldsymbol{z}))+\sum_{i=1}^{D}\text{log}\left|S_{ii}\right|\\&=\sum_{i=1}^{D}\left[ \text{log}(p_{Z_i}(\boldsymbol{z}))+\text{log}\left|S_{ii}\right|\right]

      \end{aligned}$$</p>  
  </dt-article>

  <!-- Appendix -->
  
    <dt-appendix class="centered">
<h3 id="citation">Errors and Correction</h3>
<p>Please email me at kkrzkrk@gmail.com</p>
<h3 id="citation">Citations and Reuse</h3>
<p>Diagrams and text are licensed under Creative Commons Attribution <a
        href="https://creativecommons.org/licenses/by/2.0/">CC-BY 2.0</a>. The figures that have been reused
    from other sources don't fall under this license and can be recognized by a note in their caption: “Figure
    from …”.</p>
<p>For attribution in academic contexts, please cite this work as</p>
<pre class="citation short">Arpiandi, Kiki Rizki, "Transforming Simple Distribution with Normalizing Flow", 2022.</pre>

<p>BibTeX citation</p>

<pre class="citation long">@article
{ 
  kiki2022flow,
  author = {Arpiandi, Kiki Rizki},
  title = { Transforming Simple Distribution with Normalizing Flow },
  year = {2022},
  url = {https://kikirizki.github.io/gan.html}
}</pre>
</dt-appendix>

 
<script type="text/bibliography">
  @misc{https://doi.org/10.48550/arxiv.1410.8516,
    doi = {10.48550/ARXIV.1410.8516},
    
    url = {https://arxiv.org/abs/1410.8516},
    
    author = {Dinh, Laurent and Krueger, David and Bengio, Yoshua},
    
    keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
    
    title = {NICE: Non-linear Independent Components Estimation},
    
    publisher = {arXiv},
    
    year = {2014},
    
    copyright = {arXiv.org perpetual, non-exclusive license}
  }
</script>
  <!-- jQuery (necessary for Bootstrap's JavaScript plugins)-->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
<!-- Include all compiled plugins (below), or include individual files as needed-->
<script src="/assets/js/bootstrap.min.js"></script>
<script src="/assets/js/scroll.js"></script>
<div id="disqus_thread"></div>
<script>
   /**
   *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
   *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
   /*
   var disqus_config = function () {
   this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
   */
   (function () { // DON'T EDIT BELOW THIS LINE
      var d = document, s = d.createElement('script');
      s.src = 'https://kiki-rizki-arpiandi.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
   })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by
      Disqus.</a></noscript>
</body>

</html>